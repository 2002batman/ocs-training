
= Lab: Deploying and Managing OpenShift Container Storage with Rook-Ceph Operator

== Lab Overview

This hands-on workshop is for both system administrators and application developers interested in learning how to deploy and manage OpenShift Container Storage (OCS). In this lab you will be using OpenShift Container Platform (OCP) 4.x and the OCS operator to deploy Ceph and the Multi-Cloud-Gateway (MCG) as a persistent storage solution for OCP workloads.

=== In this lab you will learn how to

* Configure and deploy containerized Ceph and Noobaa
* Validate deployment of containerized Ceph Luminous and Noobbaa
* Deploy the Rook toolbox to run Ceph and RADOS commands
* Creating a Read-Write-Once (RWO) PVC that is based on Ceph RBDs
* Creating a Read-Write-Many (RWX) PVC that is based on CephFS
* Using the Noobaa console to generate a bucket that is replicated on multiple endpoints
// * Upgrade Ceph version from Mimic to Nautilus using the Rook operator
// * Add more storage to the Ceph cluster

.Rook and Kubernetes Architecture
image::rook_diagram_3.png[Showing the Rook architecture]

.Ceph deployed on OpenShift using Rook
image::Rook_diagram_4.png[Ceph on Openshift using Rook]

[[labexercises]]
:numbered:
:language: bash
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== Deploy your storage backend using the OCS operator

=== Scale OCP cluster and add 3 new nodes

In this section, you will validate the OCP environment has 3 master and 3 worker nodes before increasing the cluster size by 3 worker nodes. The `NAME` of your OCP nodes will be different than shown below.

[source,role="execute"]
----
oc get nodes
----
----
NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-135-157.ec2.internal   Ready    worker   38m   v1.14.6+c07e432da
ip-10-0-138-253.ec2.internal   Ready    master   42m   v1.14.6+c07e432da
ip-10-0-157-189.ec2.internal   Ready    master   42m   v1.14.6+c07e432da
ip-10-0-159-240.ec2.internal   Ready    worker   38m   v1.14.6+c07e432da
ip-10-0-164-70.ec2.internal    Ready    master   42m   v1.14.6+c07e432da
----

Now you are going to add 3 more OCP compute nodes to cluster using *machinesets*.

[source,role="execute"]
----
oc get machinesets -n openshift-machine-api
----

This will show you the existing *machinesets* used to create the 3 worker nodes in the cluster already. There is a *machineset* for each AWS AZ (us-east-2a, us-east-2b, us-east-2a). Your *machinesets* `NAME` will be different than below. 

----
NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs-79cf-lj8wq-worker-us-east-1a   1         1         1       1           43m
cluster-ocs-79cf-lj8wq-worker-us-east-1b   1         1         1       1           43m
cluster-ocs-79cf-lj8wq-worker-us-east-1c   0         0                             43m
cluster-ocs-79cf-lj8wq-worker-us-east-1d   0         0                             43m
cluster-ocs-79cf-lj8wq-worker-us-east-1e   0         0                             43m
cluster-ocs-79cf-lj8wq-worker-us-east-1f   0         0                             43m
----

To create the 3 new worker nodes with available storage you will download 3 more *machineset* definition files.

[source,role="execute"]
----
curl -O https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/cluster-workerocs-us-east-1a.yaml
curl -O https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/cluster-workerocs-us-east-1b.yaml
curl -O https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/cluster-workerocs-us-east-1c.yaml
----

The files just downloaded do not have the correct `cluster-api-cluster` label for your lab because every environment is unique.

WARNING: *Make sure you do the next step for finding your CLUSTERID*

This command will find the correct value.

[source,role="execute"]
----
CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')
echo $CLUSTERID
----

Using this correct `cluster-api-cluster` label for your lab environment modify each of the 3 *machinesets* downloaded earlier using the variable `$CLUSTERID`.

[source,role="execute"]
----
sed -i "s/cluster-28cf-t22gs/$CLUSTERID/g" cluster-workerocs-us-east-1a.yaml
sed -i "s/cluster-28cf-t22gs/$CLUSTERID/g" cluster-workerocs-us-east-1b.yaml
sed -i "s/cluster-28cf-t22gs/$CLUSTERID/g" cluster-workerocs-us-east-1c.yaml
----

Check that `cluster-api-cluster` label has been changed to `$CLUSTERID`.

[source,role="execute"]
----
grep cluster-api-cluster cluster-workerocs-us-east-1*
----

The label for `cluster-api-cluster` should now match the results of `echo $CLUSTERID` from above for all occurrences.

Now you are ready to create your 3 new OCP worker nodes using these modified *machinesets*.

[source,role="execute"]
----
oc create -f cluster-workerocs-us-east-1a.yaml
oc create -f cluster-workerocs-us-east-1b.yaml
oc create -f cluster-workerocs-us-east-1c.yaml
----

Check that you have new *machines* created. 

[source,role="execute"]
----
oc get machines -n openshift-machine-api
----

They may be in `pending` for sometime so repeat command above until they are in a `running` STATE. The `NAME` of your machines will be different than shown below. 

----
NAME                                                STATE     TYPE         REGION      ZONE         AGE
cluster-ocs-79cf-lj8wq-master-0                     running   m4.xlarge    us-east-1   us-east-1a   54m
cluster-ocs-79cf-lj8wq-master-1                     running   m4.xlarge    us-east-1   us-east-1b   54m
cluster-ocs-79cf-lj8wq-master-2                     running   m4.xlarge    us-east-1   us-east-1c   54m
cluster-ocs-79cf-lj8wq-worker-us-east-1a-xscbs      running   m4.4xlarge   us-east-1   us-east-1a   54m
cluster-ocs-79cf-lj8wq-worker-us-east-1b-qcmrl      running   m4.4xlarge   us-east-1   us-east-1b   54m
cluster-ocs-79cf-lj8wq-workerocs-us-east-1a-xmd9q   running   m5.4xlarge   us-east-1   us-east-1a   46s
cluster-ocs-79cf-lj8wq-workerocs-us-east-1b-jh6k4   running   m5.4xlarge   us-east-1   us-east-1b   46s
cluster-ocs-79cf-lj8wq-workerocs-us-east-1c-649kq   running   m5.4xlarge   us-east-1   us-east-1c   45s
----

You can see that the workerocs *machines* are using a different AWS EC2 instance type `m5.4xlarge`. This is because this EC2 instance type comes with a 75GB NVMe SSD that will be used for our storage cluster. Now you want to see if our new *machines* are added to the OCP cluster.

[source,role="execute"]
----
watch oc get machinesets -n openshift-machine-api
----

This step could take more than 5 minutes. The result of this command needs to look like below before you proceed. All new workerocs *machinesets* should have an integer, in this case `1`, filled out for all rows and under columns `READY` and `AVAILABLE`. The `NAME` of your *machinesets* will be different than shown below. 

----
NAME                                          DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs-79cf-lj8wq-worker-us-east-1a      1         1         1	      1           62m
cluster-ocs-79cf-lj8wq-worker-us-east-1b      1         1         1	      1           62m
cluster-ocs-79cf-lj8wq-worker-us-east-1c      0         0                             62m
cluster-ocs-79cf-lj8wq-worker-us-east-1d      0         0                             62m
cluster-ocs-79cf-lj8wq-worker-us-east-1e      0         0                             62m
cluster-ocs-79cf-lj8wq-worker-us-east-1f      0         0                             62m
cluster-ocs-79cf-lj8wq-workerocs-us-east-1a   1         1         1       1           8m26s
cluster-ocs-79cf-lj8wq-workerocs-us-east-1b   1         1         1       1           8m26s
cluster-ocs-79cf-lj8wq-workerocs-us-east-1c   1         1         1       1           8m25s
----

Now check to see that you have 3 new OCP worker nodes. The `NAME` of your OCP nodes will be different than shown below.

[source,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/worker
----
----
NAME                           STATUS   ROLES    AGE     VERSION
ip-10-0-131-236.ec2.internal   Ready    worker   4m32s   v1.14.6+c07e432da
ip-10-0-135-157.ec2.internal   Ready    worker   60m     v1.14.6+c07e432da
ip-10-0-145-58.ec2.internal    Ready    worker   4m28s   v1.14.6+c07e432da
ip-10-0-159-240.ec2.internal   Ready    worker   60m     v1.14.6+c07e432da
ip-10-0-164-216.ec2.internal   Ready    worker   4m35s   v1.14.6+c07e432da
----

=== Installing the OCS operator

In this section you will be using three of the worker OCP nodes to deploy OCS 4. For this you will be using a manifest file, which adds multiple items to your OCP cluster:

- The `openshift-storage` namespace
- The `local-storage` namespace
- Operator groups and sources for the OCS and local-storage operators
- An OCS subbscription

To apply this manifest, execute the following:

[source,role="execute"]
----
oc apply -f https://raw.githubusercontent.com/openshift/ocs-operator/release-4.2/deploy/deploy-with-olm.yaml
----

This will fetch the manifest from the `release-4.2` tag. After applying this, you should be able to list your new operators:

[source,role="execute"]
----
watch oc -n openshift-storage get csv
----
----
NAME                            DISPLAY                                VERSION   REPLACES   PHASE
local-storage-operator.v4.2.0   Local Storage                          4.2.0                Installing
ocs-operator.v0.0.1             Openshift Container Storage Operator   0.0.1                InstallReady
----

`Csv` is a shortened word for `clusterserviceversions.operators.coreos.com`. Please wait until the operator `PHASE` changes to `Suceeded` - this will mark that the installation of your operators was successful. Reaching this state can take several minutes.

You will now also see some new pods in the new `openshift-storage` namespace:

[source,role="execute"]
----
oc -n openshift-storage get pods
----
----
NAME                                     READY   STATUS    RESTARTS   AGE
local-storage-operator-bcfd5765f-7bd86   1/1     Running   0          3m33s
noobaa-operator-7c55776bf9-kbcjp         1/1     Running   0          3m16s
ocs-operator-967957d84-9lc76             1/1     Running   0          3m16s
rook-ceph-operator-8444cfdc4c-9jm8p      1/1     Running   0          3m16s
----

Now switch over to your Openshift Web console. For me, the URL is https://console-openshift-console.apps.cluster-berlin-4c88.berlin-4c88.example.opentlc.com for you this will be slightly different.

You get your URL by issuing:

[source,role="execute"]
----
oc get -n openshift-console route  console
----

Once you are logged in, extend the `Operators` menu on the left and select `Installed Operators`. Make sure the selected project is set to `openshift-storage`.
What you see, should be similar to the following example picture:

.Installed operators - 1) Make sure you are in the right project 2) Check Operator status 3) Click on Openshift Container Storage Operator
image::OCP-installed-operators.jpg[Openshift showing the installed operators in namespace openshift-storage]

Click on `Openshift Container Storage Operator` to get to the OCS configuration screen.

.OCS configuration screen
image::OCS-config-screen.jpg[OCS configuration screen]

On the OCS configuration screen, scroll down to the box labelled `Storage cluster` and click on `Create Instance`.

.OCS create a new storage cluster
image:OCS-create-storage-cluster.jpg[OCS create storage cluster screen]

In this dialog, select three nodes that have the role `worker` and click on the button `Create` on the end of the page.

CAUTION: Make sure to select three workers in different availability zones. The OCS operator will automatically try to replicate data between those availability zones.

In the background this will start initiating a lot of new pods in the `openshift-storage` namespace, as can be seen on the CLI:

[source,role="execute"]
----
oc get po -n openshift-storage
----
.Example of a pending installation of the OCS storage cluster:
----
NAME                                            READY   STATUS                  RESTARTS   AGE
csi-cephfsplugin-2frxn                          3/3     Running                 0          57s
csi-cephfsplugin-6ghk7                          3/3     Running                 0          58s
csi-cephfsplugin-ds6zl                          3/3     Running                 0          58s
csi-cephfsplugin-j5ddw                          3/3     Running                 0          58s
csi-cephfsplugin-provisioner-57f65684f4-4sf4p   4/4     Running                 0          58s
csi-cephfsplugin-provisioner-57f65684f4-rl65b   4/4     Running                 0          58s
csi-rbdplugin-6z7qm                             3/3     Running                 0          58s
csi-rbdplugin-kxq99                             3/3     Running                 0          58s
csi-rbdplugin-provisioner-54985c744b-66fvc      5/5     Running                 0          58s
csi-rbdplugin-provisioner-54985c744b-pqwqp      5/5     Running                 0          58s
csi-rbdplugin-sdb56                             3/3     Running                 0          58s
csi-rbdplugin-t876t                             3/3     Running                 0          58s
local-storage-operator-bcfd5765f-7bd86          1/1     Running                 0          91m
noobaa-core-0                                   0/2     Pending                 0          57s
noobaa-operator-7c55776bf9-kbcjp                1/1     Running                 0          91m
ocs-operator-967957d84-9lc76                    0/1     Running                 0          91m
rook-ceph-detect-version-lh6jx                  0/1     Pending                 0          52s
rook-ceph-operator-8444cfdc4c-9jm8p             1/1     Running                 0          91m
----

You can also watch this inside of the Openshift Web Console by going back to the OCS configuration screen and selecting `All instances`.

Please wait until all Pods are marked as `Running` in the CLI or until you see all instances as `Ready` in the Web Console.

.OCS instance overview after cluster install is finished
image:OCS-finished-cluster-install.png[OCS instance overview after cluster install is finished]

[source,role="execute"]
----
oc -n openshift-storage get pods
----
.Output when the cluster installation is finished
----
NAME                                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-6975g                                            3/3     Running     0          24m
csi-cephfsplugin-ckpk4                                            3/3     Running     0          24m
csi-cephfsplugin-h6j7j                                            3/3     Running     0          24m
csi-cephfsplugin-provisioner-57f65684f4-dk5bv                     4/4     Running     0          24m
csi-cephfsplugin-provisioner-57f65684f4-nwsws                     4/4     Running     0          24m
csi-cephfsplugin-t9rvk                                            3/3     Running     0          24m
csi-rbdplugin-jhj8v                                               3/3     Running     0          24m
csi-rbdplugin-k6bs2                                               3/3     Running     0          24m
csi-rbdplugin-nqmbl                                               3/3     Running     0          24m
csi-rbdplugin-provisioner-54985c744b-4sxvv                        5/5     Running     0          24m
csi-rbdplugin-provisioner-54985c744b-xtlv9                        5/5     Running     0          24m
csi-rbdplugin-wwdkb                                               3/3     Running     0          24m
local-storage-operator-bcfd5765f-j6x7m                            1/1     Running     0          26m
noobaa-core-0                                                     2/2     Running     0          24m
noobaa-operator-7c55776bf9-89cxn                                  1/1     Running     0          26m
ocs-operator-967957d84-cmksd                                      1/1     Running     0          26m
rook-ceph-drain-canary-ip-10-0-131-104-5b49b94554-8wwjl           1/1     Running     0          21m
rook-ceph-drain-canary-ip-10-0-150-178-54f44b45fd-zxrhp           1/1     Running     0          21m
rook-ceph-drain-canary-ip-10-0-175-125-7bf8fc5d79-bg8lq           1/1     Running     0          21m
rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-577b9f85xzlvj   1/1     Running     0          21m
rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-55768bc8r6wsd   1/1     Running     0          20m
rook-ceph-mgr-a-6b9b8d4bf6-vhr9h                                  1/1     Running     0          22m
rook-ceph-mon-a-5846c784b-jzr6l                                   1/1     Running     0          24m
rook-ceph-mon-b-c8858957-4xcbq                                    1/1     Running     0          23m
rook-ceph-mon-c-54979d9856-llbsk                                  1/1     Running     0          22m
rook-ceph-operator-8444cfdc4c-nmr2q                               1/1     Running     0          26m
rook-ceph-osd-0-77d8884557-jwslr                                  1/1     Running     0          21m
rook-ceph-osd-1-54d6d78694-47ghl                                  1/1     Running     0          21m
rook-ceph-osd-2-796d848bd7-jb825                                  1/1     Running     0          21m
rook-ceph-osd-prepare-ocs-deviceset-0-0-8fls2-p7pd5               0/1     Completed   0          22m
rook-ceph-osd-prepare-ocs-deviceset-1-0-lbrls-ztgfs               0/1     Completed   0          22m
rook-ceph-osd-prepare-ocs-deviceset-2-0-4ktq4-zhgcr               0/1     Completed   0          22m
rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-66499c5gt8q4   1/1     Running     0          4m23s
----

You can now also check the status of your storage cluster with the Dashboard that is included in your Openshift Web Console. You can reach this by clicking on `Home` on your left navigation bar, then selecting `Dashboards` and finally clicking on `Persistent Storage` on the top navigation bar of the content page.

.OCS Dashboard after successful backing storage installation
image:OCS-dashboard-healthy.png[OCS Dashboard after successful backing storage installation]

OCS ships with a Dashboard for the Object Store as well. From within the `Dashboard` menu click on the `Object Service` on the top navigation bar of the content page.

.OCS Multi-Cloud-Gateway Dashboard after successful installation
image:OCS-noobaa-dashboard-healthy.png[OCS Multi-Cloud-Gateway Dashboard after successful installation]

On the left side of the dashboard you see a blue link labelled `noobaa`, which will get you to the Noobaa Management Console. We will discuss this Management Console later in more detail.

Once this is all healthy, you will be able to use the three new StorageClasses:

- ocs-storagecluster-ceph-rbd
- ocs-storagecluster-cephfs
- openshift-storage.noobaa.io

You can see them in the Openshift Web Console by expanding the `Storage` menu in the left navigation bar and selecting `Storage Classes`. Please make sure the three storage classes are available in your cluster.

NOTE: The Noobaa pod is already using the `ocs-storagecluster-ceph-rbd` storage class for its internal database

=== Using the Ceph toolbox to check on the Ceph backing storage

Since the Ceph toolbox is not shipped with OCS, we need to deploy it manually. For this, we can leverage the upstream Rook toolbox, but we need to modify the namespace.
You can use this one-liner to deploy the toolbox directly:

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/rook/rook/release-1.1/cluster/examples/kubernetes/ceph/toolbox.yaml | sed 's/namespace: rook-ceph/namespace: openshift-storage/g'| oc apply -f -
----

Afterwards you can work inside of the toolbox like this:

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

Once inside, try out the following commands:

----
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
----

You can exit the toolbox by either pressing kbd:[Ctrl+D] or by executing `exit`

// sh-4.2# ceph status
//   cluster:
//     id:     f73cd30a-b37c-46e9-8b5f-37931f165857
//     health: HEALTH_OK
 
//   services:
//     mon: 3 daemons, quorum a,b,c (age 27m)
//     mgr: a(active, since 27m)
//     mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-b=up:active} 1 up:standby-replay
//     osd: 3 osds: 3 up (since 26m), 3 in (since 26m)
//     rgw: 1 daemon active (ocs.storagecluster.cephobjectstore.a)
 
//   data:
//     pools:   9 pools, 72 pgs
//     objects: 321 objects, 192 MiB
//     usage:   3.4 GiB used, 3.0 TiB / 3.0 TiB avail
//     pgs:     72 active+clean
 
//   io:
//     client:   1.2 KiB/s rd, 19 KiB/s wr, 2 op/s rd, 2 op/s wr
 
// sh-4.2# ceph osd status
// +----+-----------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
// | id |                      host                     |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
// +----+-----------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
// | 0  | ip-10-0-131-104.eu-central-1.compute.internal | 1161M | 1021G |    1   |  4915   |    3   |   106   | exists,up |
// | 1  | ip-10-0-175-125.eu-central-1.compute.internal | 1161M | 1021G |    0   |     0   |    0   |     0   | exists,up |
// | 2  | ip-10-0-150-178.eu-central-1.compute.internal | 1161M | 1021G |    0   |     0   |    0   |     0   | exists,up |
// +----+-----------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
// sh-4.2# ceph osd tree
// ID CLASS WEIGHT  TYPE NAME                        STATUS REWEIGHT PRI-AFF 
// -1       2.99698 root default                                             
// -7       0.99899     host ocs-deviceset-0-0-8fls2                         
//  2   ssd 0.99899         osd.2                        up  1.00000 1.00000 
// -5       0.99899     host ocs-deviceset-1-0-lbrls                         
//  1   ssd 0.99899         osd.1                        up  1.00000 1.00000 
// -3       0.99899     host ocs-deviceset-2-0-4ktq4                         
//  0   ssd 0.99899         osd.0                        up  1.00000 1.00000 
// sh-4.2# ceph df
// RAW STORAGE:
//     CLASS     SIZE        AVAIL       USED        RAW USED     %RAW USED 
//     ssd       3.0 TiB     3.0 TiB     412 MiB      3.4 GiB          0.11 
//     TOTAL     3.0 TiB     3.0 TiB     412 MiB      3.4 GiB          0.11 
 
// POOLS:
//     POOL                                                     ID     STORED      OBJECTS     USED        %USED     MAX AVAIL 
//     ocs-storagecluster-cephblockpool                          1     135 MiB          97     407 MiB      0.01       971 GiB 
//     ocs-storagecluster-cephobjectstore.rgw.control            2         0 B           8         0 B         0       971 GiB 
//     ocs-storagecluster-cephfilesystem-metadata                3     2.2 KiB          22     384 KiB         0       971 GiB 
//     ocs-storagecluster-cephfilesystem-data0                   4         0 B           0         0 B         0       971 GiB 
//     ocs-storagecluster-cephobjectstore.rgw.meta               5         0 B           0         0 B         0       971 GiB 
//     ocs-storagecluster-cephobjectstore.rgw.log                6        50 B         178      48 KiB         0       971 GiB 
//     ocs-storagecluster-cephobjectstore.rgw.buckets.index      7         0 B           0         0 B         0       971 GiB 
//     .rgw.root                                                 8     4.6 KiB          16     720 KiB         0       971 GiB 
//     ocs-storagecluster-cephobjectstore.rgw.buckets.data       9         0 B           0         0 B         0       971 GiB 
// sh-4.2# rados df
// POOL_NAME                                               USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS      WR USED COMPR UNDER COMPR 
// .rgw.root                                            720 KiB      16      0     48                  0       0        0     46  52 KiB     32  26 KiB        0 B         0 B 
// ocs-storagecluster-cephblockpool                     407 MiB      97      0    291                  0       0        0    143 1.3 MiB   1804 150 MiB        0 B         0 B 
// ocs-storagecluster-cephfilesystem-data0                  0 B       0      0      0                  0       0        0      0     0 B      0     0 B        0 B         0 B 
// ocs-storagecluster-cephfilesystem-metadata           384 KiB      22      0     66                  0       0        0   3116 1.5 MiB     45  13 KiB        0 B         0 B 
// ocs-storagecluster-cephobjectstore.rgw.buckets.data      0 B       0      0      0                  0       0        0      0     0 B      0     0 B        0 B         0 B 
// ocs-storagecluster-cephobjectstore.rgw.buckets.index     0 B       0      0      0                  0       0        0      0     0 B      0     0 B        0 B         0 B 
// ocs-storagecluster-cephobjectstore.rgw.control           0 B       8      0     24                  0       0        0      0     0 B      0     0 B        0 B         0 B 
// ocs-storagecluster-cephobjectstore.rgw.log            48 KiB     178      0    534                  0       0        0    544 368 KiB    389  37 KiB        0 B         0 B 
// ocs-storagecluster-cephobjectstore.rgw.meta              0 B       0      0      0                  0       0        0      0     0 B      0     0 B        0 B         0 B 

// total_objects    321
// total_used       3.4 GiB
// total_avail      3.0 TiB
// total_space      3.0 TiB

=== Change the default storage class to Ceph RBD

After installing OCS, it is best practice to change the default storage class from AWS gp2 to our new OCS-backed storage class `ocs-storagecluster-ceph-rbd`.
The easiest way to do this is using the Openshift Web Console. In there, expand the `Storage` item on the left navigation bar and select `Storage Classes`.

.OCP Storage classes after OCS installation - AWS gp2 is the default storage class
image:OCS-Storage-Classes-gp2-default.png[]

Now click on the three dots next to the gp2 storage class and select `Edit Annotations`:

image:OCS-edit-gp2-annotations.png[]

click on the stop sign on the right to delete the only one entry. Proceed by clicking on `Save`.

Now click on the three dots next to the ocs-storagecluster-ceph-rbd storage class and select `Edit Annotations`
In the new window enter `storageclass.kubernetes.io/is-default-class` as the Key and `true` as the value of the new annotation. Proceed by clicking on `Save`.

Now the `ocs-storagecluster-ceph-rbd` storage class should be marked as default, like in the below picture:

.OCP Storage classes after OCS installation - Ceph RBD is now the default storage class
image:OCS-Storage-Classes-rbd-default.png[]

WARNING: End of current refresh work

== Create a new OCP deployment using Ceph RBD volume

In this section the `ocs-storagecluster-ceph-rbd` *storage class* will be used by an OCP application + database deployment to create persistent storage. The persistent storage will be a Ceph RBD (RADOS Block Device) volume (object) in the Ceph pool `ocs-storagecluster-cephblockpool`.

Make sure that you completed all previous sections so that you are ready to start the Rails + PostgreSQL deployment.

[source,role="execute"]
----
oc new-project my-database-app
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
----

After the deployment is started you can monitor with these commands.

[source,role="execute"]
----
oc status
oc get pvc -n my-database-app
watch oc get pods -n my-database-app
----

This step could take 5 or more minutes. Wait until there are 2 pods in `Running` STATUS and 4 pods in `Completed` STATUS as shown below.

[source,role="execute"]
----
watch oc get pods -n my-database-app
----
----
NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          3m36s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          3m28s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          3m14s
----

Once the deployment is complete you can now test the application and the persistent storage on Ceph. Your `HOST/PORT` will be different.

[source,role="execute"]
----
oc get route -n my-database-app
----
----
NAME                     HOST/PORT                                                                         PATH   SERVICES                 PORT    TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com          rails-pgsql-persistent
----

Copy your `HOST/PORT` to a browser window to create articles. You will need to append `/articles` to the end.

*Example link:*  http://rails-pgsql-persistent-my-database-app.apps.cluster-a26e.sandbox449.opentlc.com /articles

Enter the `username` and `password` below to create articles and comments. The articles and comments are saved in a PostgreSQL database which stores its table spaces on the Ceph RBD volume provisioned using the `ocs-storagecluster-ceph-rbd` *storagclass* during the application deployment.

[source,ini]
----
username: openshift
password: secret
----

Lets now take another look at the Ceph `ocs-storagecluster-cephblockpool` created by the `ocs-storagecluster-ceph-rbd` *storageclass*. Log into the *toolbox* pod again.

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

Run the same Ceph commands as before the application deployment and compare to results in prior section. Notice the number of objects in `ocs-storagecluster-cephblockpool` now.

[source,role="execute"]
----
ceph df
rados df
rbd -p ocs-storagecluster-cephblockpool ls | grep vol
----

Make sure to `exit` the *toolbox*. Validate the OCP *PersistentVolume* (PV) name is similar to the volume name in the Ceph `ocs-storagecluster-cephblockpool`.

[source,role="execute"]
----
oc get pvc -n my-database-app
----

== Create a new OCP deployment using CephFS

In this section the `ocs-storagecluster-cephfs` will be used to create a RWX PVC that we can attach to multiple pods at the same time. As an example we will be running a highly-available container image registry. The persistent storage will be based on a CephFS volume in the Ceph pool `ocs-storagecluster-cephfilesystem-data0`.

Make sure you completed the setup sections, so that you have the `ocs-storagecluster-cephfs` available.

Deploy the registry like this:

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml | sed 's/storageClassName: csi-cephfs/storageClassName: ocs-storagecluster-cephfs/g'| oc apply -f - 
----

This will create a PVC in the `kube-system` namespace:

[source,role="execute"]
----
oc get pvc -n kube-system
----
----
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
cephfs-pvc   Bound    pvc-a7015af1-f0dd-11e9-8812-06aa2fd1035a   1Gi        RWX            ocs-storagecluster-cephfs   55s
----

As well as a deployment for our registry:

[source,role="execute"]
----
oc get -n kube-system deployment
----
----
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
kube-registry   3/3     3            3           100s
----

.The `kube-registry` deployment consists of more than one replica
NOTE: Since our deployment consists of three containers, we need a RWX PVC, so that all replicas can access the persistent volume in parallel.

Let's make our deployment accessible. First we create a service with an internal Cluster IP:

[source,role="execute"]
----
oc expose -n kube-system deployment kube-registry
----

and then we create a route with a edge termination so that it serves our registry with tls:

[source,role="execute"]
----
oc create route edge -n kube-system --service=kube-registry
----

Then we check on our new route to get the URL for our registry:

[source,role="execute"]
----
oc get route
----
----
NAME            HOST/PORT                                                                            PATH   SERVICES        PORT    TERMINATION   WILDCARD
kube-registry   kube-registry-kube-system.apps.cluster-berlin-fc41.berlin-fc41.example.opentlc.com          kube-registry   <all>   edge          None
----

We will now download the alpine container image as an example and upload it to our new registry:

[source,role="execute"]
----
podman pull docker.io/library/alpine
podman push docker.io/library/alpine kube-registry-kube-system.apps.cluster-berlin-fc41.berlin-fc41.example.opentlc.com/alpine
----

CAUTION: Make sure to replace the URL in the push command with the URL of your route

Next we use the toolbox pod to check on our underlying CephFS volume:

[source,role="execute"]
----
# Create the directory
mkdir /tmp/registry

# Detect the mon endpoints and the user secret for the connection
mon_endpoints=$(grep mon_host /etc/ceph/ceph.conf | awk '{print $3}')
my_secret=$(grep key /etc/ceph/keyring | awk '{print $3}')

# Mount the file system
mount -t ceph -o mds_namespace=ocs-storagecluster-cephfilesystem,name=admin,secret=$my_secret $mon_endpoints:/ /tmp/registry

# Find our uploaded container image
ls /tmp/registry/volumes/csi/csi-vol-*/docker/registry/v2/repositories/alpine/

# See your mounted file system
df -h
----

== Upgrading the Ceph version

== Adding storage to the Ceph Cluster

WARNING: This section has not been rewritten for OCS and still contains Rook steps!

In this section you will add more storage to the cluster by increasing the number of OCP workerocs *machines* and worker nodes from 3 to 4 using one of the *machinesets* you already used. The new *machine* will also be an EC2 instance `m5d.large` and have an available 75 GB NVMe SSD. The Rook operator will `observe` when the new OCP node is added to the cluster and will then create a new *OSD* pod on this new worker node and the 75 GB SSD will be added to the Ceph cluster with no additional manual effort from the user.

To increase the number of *machines* and the OCP nodes you will again use a *machineset*. Each of the *machinesets* you used earlier created just one machine because of `replicas: 1` in the configuration file. Your `cluster-api-cluster` and `name` is different than example shown below.

[source,role="execute"]
----
cat machineset cluster-workerocs-us-east-2a.yaml | more
----
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-a26e-rx8bk
    machine.openshift.io/cluster-api-machine-role: workerocs
    machine.openshift.io/cluster-api-machine-type: workerocs
  name: cluster-a26e-rx8bk-workerocs-us-east-2a
  namespace: openshift-machine-api
spec:
  replicas: 1
...

----

Verify your `cluster-api-cluster` again by using the command below.

[source,role="execute"]
----
echo $CLUSTERID
----

You can easily create a new *machine* and OCP worker node in AWS AZ us-east-2a by just increasing the `replicas` count in one of the machinesets. Edit your machineset for us-east-2a to increase from `replicas: 1` to `replicas: 2`. Make sure to save your changes before exiting `:wq!`.

[source,role="execute"]
----
oc edit machineset $CLUSTERID-workerocs-us-east-2a -n openshift-machine-api
----

Verify you now have 4 workerocs *machines*. One of the *machines* should have just been created in us-east-2a AZ so there are two in this AZ now. The `NAME` of your *machines* will be different than shown below.

[source,role="execute"]
----
oc get machines -n openshift-machine-api
----
----
NAME                                            INSTANCE              STATE     TYPE         REGION      ZONE         AGE

cluster-a26e-rx8bk-workerocs-us-east-2a-8pnf4   i-0a497998c19a59ba3   running   m5d.large    us-east-2   us-east-2a   2d
cluster-a26e-rx8bk-workerocs-us-east-2a-l4v5l   i-0e22f1078f1228086   running   m5d.large    us-east-2   us-east-2a   33s
cluster-a26e-rx8bk-workerocs-us-east-2b-wwcmd   i-0c25eb473e452645d   running   m5d.large    us-east-2   us-east-2b   47h
cluster-a26e-rx8bk-workerocs-us-east-2c-8456v   i-0e0d311e4590fa7e3   running   m5d.large    us-east-2   us-east-2c   47h
----

Now you need to verify that this new *machine* is added to the OCP cluster.

This step could take more than 5 minutes. You can see now that one of the *machinesets* has 2 machines, this is because you increased the replica count in the prior step. The *machineset* for us-east-2a should have an integer, in this case `2`, filled out for the entire row before you proceed to the next step. The `NAME` of your machinesets will be different than shown below.

[source,role="execute"]
----
watch oc get machinesets -n openshift-machine-api
----
----
NAME                                      DESIRED   CURRENT   READY   AVAILABLE   AGE
...
cluster-a26e-rx8bk-workerocs-us-east-2a   2         2         2       2           2d
cluster-a26e-rx8bk-workerocs-us-east-2b   1         1         1       1           2d
cluster-a26e-rx8bk-workerocs-us-east-2c   1         1         1       1           2d
----

Now verify that you have a new OCP worker node. You should now have 7 worker nodes.

[source,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/worker
----
----
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-0-135-6.us-east-2.compute.internal     Ready    worker   2d      v1.13.4+da48e8391
ip-10-0-135-64.us-east-2.compute.internal    Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-137-156.us-east-2.compute.internal   Ready    worker   4m28s   v1.13.4+da48e8391
ip-10-0-146-50.us-east-2.compute.internal    Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-156-83.us-east-2.compute.internal    Ready    worker   2d      v1.13.4+da48e8391
ip-10-0-160-232.us-east-2.compute.internal   Ready    worker   2d2h    v1.13.4+da48e8391
ip-10-0-164-65.us-east-2.compute.internal    Ready    worker   2d      v1.13.4+da48e8391
----

Until Openshift Container Platform 4.2 rolls out, we will need to restart (delete) the operator pod to see OSD pod added.
[source,bash,role="execute"]
----
oc delete pod -l app=rook-ceph-operator -n rook-ceph
----

This step could take 5 minutes or more for the forth *OSD* pod to be in a `Running` STATUS. Eventually your will see a new *OSD* pod, `rook-ceph-osd-3`, that has just been created.

[source,role="execute"]
----
watch oc get pods -n rook-ceph
----
----
NAME                                          READY   STATUS      RESTARTS   AGE

...
rook-ceph-osd-0-855bc669cd-45sk7              1/1     Running     0          55m
rook-ceph-osd-1-7cc9cd8c8c-j9ffl              1/1     Running     0          55m
rook-ceph-osd-2-5977cd8bff-9x85n              1/1     Running     0          55m
rook-ceph-osd-3-56b6c4f459-q7mhz              1/1     Running     0          114s
...

----

Now you can validate that Ceph is healthy and has the additional storage. You again login to the *toolbox* pod.

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

And run Ceph commands to see the new OSDs.

[source,role="execute"]
----
ceph osd status
----
----
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| id |                    host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  |  ip-10-0-135-6.us-east-2.compute.internal  | 1051M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 1  | ip-10-0-156-83.us-east-2.compute.internal  | 1060M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 2  | ip-10-0-164-65.us-east-2.compute.internal  | 1062M | 68.8G |    0   |     0   |    0   |     0   | exists,up |
| 3  | ip-10-0-137-156.us-east-2.compute.internal | 1061M | 67.9G |    0   |     0   |    0   |     0   | exists,up |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
----

And you can see that Ceph is healthy and happy! There are now 4 *OSDs* `up` and `in`. You might even want to go back to the the Rails + PostgreSQL application and save a few more articles to make sure applications using Ceph storage are still working.

[source,role="execute"]
----
ceph status
----
----
  cluster:
    id:     90306026-6e42-4877-9d4e-26eca2ecf6ef
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum b,a,c (age 59m)
    mgr: a(active, since 5m)
    osd: 4 osds: 4 up, 4 in

  data:
    pools:   1 pools, 100 pgs
    objects: 36 objects, 73 MiB
    usage:   4.1 GiB used, 274 GiB / 279 GiB avail
    pgs:     100 active+clean
----

Make sure to `exit` the *toolbox*.

== Using must-gather

Must-gather is a tool for collecting data about the current'y running Openshift cluster. It loads a predefined set of containers that execute multiple programs and dump it on the local workstations filesystem.
The local files can then be used by a remote support engineer to debug a problem more easily without needing direct cluster access. This is similar to sosreports for RHEL hosts.

The OCS team has released its own image for the must-gather tool that runs storage specific commands.

You can run this diagnostic tool like this for generic Openshift debugging:

----
oc adm must-gather
----

Or like this for OCS specific insights:

----
oc adm must-gather --image=quay.io/ocs-dev/ocs-must-gather
----

The output will then be saved in the current directory inside of a new folder called `must-gather.local.(random)`

More runtime options can be displayed with

----
oc adm must-gather -h
----
----
Launch a pod to gather debugging information

 This command will launch a pod in a temporary namespace on your cluster that gathers debugging information and then
downloads the gathered information.

 Experimental: This command is under active development and may change without notice.

Usage:
  oc adm must-gather [flags]

Examples:
  # gather information using the default plug-in image and command, writing into ./must-gather.local.<rand>
  oc adm must-gather
  
  # gather information with a specific local folder to copy to
  oc adm must-gather --dest-dir=/local/directory
  
  # gather information using multiple plug-in images
  oc adm must-gather --image=quay.io/kubevirt/must-gather --image=quay.io/openshift/origin-must-gather
  
  # gather information using a specific image stream plug-in
  oc adm must-gather --image-stream=openshift/must-gather:latest
  
  # gather information using a specific image, command, and pod-dir
  oc adm must-gather --image=my/image:tag --source-dir=/pod/directory -- myspecial-command.sh

Options:
      --dest-dir='': Set a specific directory on the local machine to write gathered data to.
      --image=[]: Specify a must-gather plugin image to run. If not specified, OpenShift's default must-gather image
will be used.
      --image-stream=[]: Specify an image stream (namespace/name:tag) containing a must-gather plugin image to run.
      --node-name='': Set a specific node to use - by default a random master will be used
      --source-dir='/must-gather/': Set the specific directory on the pod copy the gathered data from.

Use "oc adm options" for a list of global command-line options (applies to all commands).
----

// On the Openshift side must-gather has nowadays been replaced by `oc adm inspect`.

== Using the MCG Management Console

Some unsorted notes:

Get the Noobaa Management Console access credentials:

[source,role="execute"]
----
oc get secret noobaa-admin -n openshift-storage -o json | jq '.data|map_values(@base64d)'
----
[source,json,indent=5]
----
{
  "AWS_ACCESS_KEY_ID": "5LqXmAljVYdcX6KVOOc5",
  "AWS_SECRET_ACCESS_KEY": "7x9R895zSk7xhD+CP+w1ePvc4m6018F7aD4/W156",
  "email": "admin@noobaa.io",
  "password": "wLg04tfKYnUvC66WFd0p8Q==",
  "system": "noobaa"
}
----

* By default there is no OCP backing storage for Noobaa, but an AWS bucket
* Creating Noobaa storage on OCP works, creates new Pods with default storage class
* Noobaa dashboard reports "Multi cloud gateway is not running" after adding OCP backend storage...
* Noobaa uses self-signed certificate even though OCP uses LetsEncrypt
* OCP backing storage should be > 30GB to not trigger warning
